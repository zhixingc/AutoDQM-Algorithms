{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.20/06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ROOT\n",
    "import uproot\n",
    "import uproot_methods\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Training of Histogram Data for AutoDQM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook pulls data through EOS on SWAN to train PCA models for significant histograms in the CSC subsystem. This notebook is adapted from Bennett Marsh's [CSC-DQM-ML repository](https://github.com/bjmarsh/CSC-DQM-ML). In the current version, the data is queried differently, uproot is used instead of ROOT, and changes to the histogram classes were made for correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variables for bulk training\n",
    "year = 2016\n",
    "norm_cut = 100000\n",
    "max_bins = None\n",
    "title = None\n",
    "lumi_json = None\n",
    "hpath = \"DQMData/Run {}/CSC/Run summary/CSCOfflineMonitor\"\n",
    "plots = [(\"Digis\",\"hWireTBin_p11b\"),(\"Segments\",\"hSnSegments\"),(\"recHits\",\"hRHTimingAnodem11a\"),\n",
    "          (\"recHits\",\"hRHTimingm22\"),(\"Resolution\",\"hSResidp12\"),(\"BXMonitor\",\"hCLCTL1A\"),(\"Segments\",\"hSGlobalTheta\"),\n",
    "          (\"recHits\",\"hRHSumQm11a\"),(\"recHits\",\"hRHnrechits\"),(\"Digis\",\"hWireTBin_p32\"),(\"Segments\",\"hSGlobalPhi\"),(\"Segments\",\"hSTimeCombined\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HistogramIntegral returns the total number of events\n",
    "def HistogramIntegral(hist):\n",
    "    return sum(hist[0][i] for i in range(len(hist[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "To avoid repeated datapoints based from different stages of reconstruction, we are only training from the SingleMuon dataset and for each year, we use:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2018 - full reconstruction **17sepB** for section A-C, and **Prompt Reco** for section D of 2018 (no full reco for D).  (Note that the asterisk in the code  allows for all versions, which is correct since for reconstructions above, there is only one version for each section.)\n",
    "Specifically for 2018, we are training with:  \n",
    "17sep A v2  \n",
    "17sep B v1  \n",
    "17sep C v1  \n",
    "PR   &nbsp; &nbsp; &nbsp;D v2  \n",
    "\n",
    "2017 - full reconstruction Nov2017 for B-F\n",
    "\n",
    "2016 - UL 21Feb2020_UL for B-H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Histogram Classes and Wrapper for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistCollection(object):\n",
    "    \"\"\"Store a collection of cleaned histograms for use in ML algorithms.\"\"\"\n",
    "\n",
    "    def __init__(self, hdata, normalize=True, remove_identical_bins=True, extra_info=None, \n",
    "                 hist_cleaner=None):\n",
    "        \"\"\"\n",
    "        Initialize the HistCollection.\n",
    "        \n",
    "        hdata is a 2D array of histogram data\n",
    "          Each row is a histogram and each column a bin\n",
    "        normalize: whether or not to scale histograms to unit area\n",
    "        remove_identical_bins: remove bins that are the same in every histogram in the collection\n",
    "        extra_info: dict containing any auxiliary info you want to be stored\n",
    "          (e.g. extra_info[\"runs\"] could be a list of runs corresponding to each histogram)\n",
    "\n",
    "        The histograms will be \"cleaned\" using the HistCleaner class\n",
    "        \"\"\"\n",
    "\n",
    "        self.hdata = np.array(hdata, dtype=float)\n",
    "        self.__nhists = self.hdata.shape[0]\n",
    "        self.__nbins = self.hdata.shape[1]\n",
    "        self.norms = np.sum(hdata, axis=1)\n",
    "\n",
    "        if hist_cleaner is not None:\n",
    "            self.__hist_cleaner = hist_cleaner\n",
    "        else:\n",
    "            self.__hist_cleaner = HistCleaner(normalize, remove_identical_bins)\n",
    "        self.__hist_cleaner.fit(self.hdata)\n",
    "        self.hdata = self.__hist_cleaner.transform(self.hdata)\n",
    "        \n",
    "\n",
    "        self.shape = self.hdata.shape\n",
    "        self.extra_info = extra_info\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def nhists(self):\n",
    "        return self.__nhists\n",
    "\n",
    "    @property\n",
    "    def nbins(self):\n",
    "        return self.__nbins\n",
    "\n",
    "    @property\n",
    "    def hist_cleaner(self):\n",
    "        return self.__hist_cleaner\n",
    "\n",
    "    @staticmethod\n",
    "    def draw(h, ax=None, text=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Plot a single histogram with matplotlib.\n",
    "          - ax: the matplotlib axis to use. Defaults to plt.gca()\n",
    "          - text: string to write on the plot\n",
    "          - kwargs: keywork args to pass to pyplot.hist\n",
    "        \"\"\"\n",
    "\n",
    "        if not ax:\n",
    "            ax = plt.gca()\n",
    "\n",
    "        if \"histtype\" not in kwargs:\n",
    "            kwargs[\"histtype\"] = 'stepfilled'\n",
    "        if \"color\" not in kwargs:\n",
    "            kwargs[\"color\"] = 'k'\n",
    "        if \"linewidth\" not in kwargs and \"lw\" not in kwargs:\n",
    "            kwargs[\"lw\"] = 1\n",
    "        if \"facecolor\" not in kwargs and \"fc\" not in kwargs:\n",
    "            kwargs[\"fc\" ] = \"lightskyblue\"\n",
    "        if \"linestyle\" not in kwargs and \"ls\" not in kwargs:\n",
    "            kwargs[\"linestyle\"] = '-'\n",
    "\n",
    "        nbins = h.size\n",
    "        ax.hist(np.arange(nbins)+0.5, weights=h, bins=np.arange(nbins+1),\n",
    "                **kwargs)\n",
    "        ax.set_ylim(0, np.amax(h)*1.5)\n",
    "        if np.amax(h) > 10000/1.5:\n",
    "            ax.ticklabel_format(axis='y', style='scientific', scilimits=(0,0))\n",
    "        if text:\n",
    "            ax.text(0.05, 0.9, text, transform=ax.transAxes)\n",
    "        \n",
    "    def draw_single(self, idx, restore_bad_bins=True, use_normed=False, draw_title=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Plot the histogram at index idx with matplotlib.\n",
    "          - ax: the matplotlib axis to use. Defaults to plt.gca()\n",
    "          - restore_bad_bins: use the HistCleaner to restore bins that were removed for plotting\n",
    "          - use_normed: whether to draw normalized histograms\n",
    "          - draw_title: whether to draw the title on the plot (extra_info[\"title\"] must exist)\n",
    "          - kwargs: arguments to pass to the draw function above\n",
    "        \"\"\"\n",
    "\n",
    "        h = self.hdata[idx, :]\n",
    "        if restore_bad_bins:\n",
    "            h = self.__hist_cleaner.restore_bad_bins(h)\n",
    "        if not use_normed:\n",
    "            h = h*self.norms[idx]\n",
    "\n",
    "        if draw_title and \"title\" in self.extra_info:\n",
    "            kwargs[\"text\"] = self.extra_info[\"title\"]\n",
    "\n",
    "        HistCollection.draw(h, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistCleaner(object):\n",
    "    \"\"\" \n",
    "    sklearn-style preprocessing class to perform necessary \"cleaning\" of histogram collections for use in ML algorithms\n",
    "    \n",
    "    Can perform two separate operations, controlled by boolean flags:\n",
    "     - normalize: whether or not to scale histograms to unit area\n",
    "     - remove_identical_bins: remove bins that are the same in every histogram in the collection\n",
    "    \"\"\"\n",
    "    def __init__(self, normalize=True, remove_identical_bins=True):\n",
    "        self.__normalize = normalize\n",
    "        self.__remove_identical_bins = remove_identical_bins\n",
    "\n",
    "        # internal use\n",
    "        self.__is_fit = False\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def normalize(self):\n",
    "        return self.__normalize\n",
    "\n",
    "    @normalize.setter\n",
    "    def normalize(self, norm):\n",
    "        if not isinstance(norm, bool):\n",
    "            raise Exception(\"normalize must be set to a boolean value\")\n",
    "        self.__normalize = norm\n",
    "\n",
    "    @property\n",
    "    def remove_identical_bins(self):\n",
    "        return self.__remove_identical_bins\n",
    "\n",
    "    @remove_identical_bins.setter\n",
    "    def remove_identical_bins(self, rib):\n",
    "        if not isinstance(rib, bool):\n",
    "            raise Exception(\"remove_identical_bins must be set to a boolean value\")\n",
    "        self.__remove_identical_bins = rib\n",
    "\n",
    "    def fit(self, hd):\n",
    "        self.nbins = hd.shape[1]\n",
    "        # find the \"good\" bin indices (those that aren't the same in every histogram)\n",
    "        #np.tile transform and repeat a given array\n",
    "        bad_bins = np.all(hd==np.tile(hd[0,:],hd.shape[0]).reshape(hd.shape), axis=0)\n",
    "        \n",
    "        good_bins = np.logical_not(bad_bins)\n",
    "        self.bad_bins = np.arange(self.nbins)[bad_bins]\n",
    "        self.good_bins = np.arange(self.nbins)[good_bins]\n",
    "        self.n_good_bins = self.good_bins.size\n",
    "        self.bad_bin_contents = hd[0,self.bad_bins]\n",
    "\n",
    "        self.__is_fit = True\n",
    "\n",
    "    def _check_fit(self):\n",
    "        if not self.__is_fit:\n",
    "            raise Exception(\"Must fit the HistCleaner before calling transform\")\n",
    "\n",
    "    def restore_bad_bins(self, hd):\n",
    "        self._check_fit()\n",
    "        init_shape = hd.shape\n",
    "        if len(init_shape) == 1:\n",
    "            hd = hd.reshape(1,-1)\n",
    "        if hd.shape[1] != self.n_good_bins:\n",
    "            raise Exception(\"Invalid number of columns\")\n",
    "\n",
    "        ret = np.zeros((hd.shape[0], self.nbins))\n",
    "        ret[:,self.good_bins] = hd\n",
    "        ret[:,self.bad_bins] = np.tile(self.bad_bin_contents, hd.shape[0]).reshape(hd.shape[0], self.bad_bins.size)\n",
    "\n",
    "        if len(init_shape) == 1:\n",
    "            ret = ret.reshape(ret.size,)\n",
    "        return ret\n",
    "\n",
    "    def remove_bad_bins(self, hd):\n",
    "        self._check_fit() \n",
    "        init_shape = hd.shape\n",
    "        if len(init_shape) == 1:\n",
    "            hd = hd.reshape(1,-1)\n",
    "        if hd.shape[1] != self.nbins:\n",
    "            raise Exception(\"Invalid number of columns\")\n",
    "        \n",
    "        ret = hd[:,self.good_bins]\n",
    "        if len(init_shape) == 1:\n",
    "            ret = ret.reshape(ret.size,)\n",
    "        return ret\n",
    "\n",
    "    def transform(self, hd):\n",
    "        self._check_fit()\n",
    "        init_shape = hd.shape\n",
    "        if len(init_shape)==1:\n",
    "            hd = hd.reshape(1,-1)\n",
    "        is_cleaned = False\n",
    "        if hd.shape[1] != self.nbins:\n",
    "            if hd.shape[1] == self.n_good_bins:\n",
    "                is_cleaned = True\n",
    "            else:\n",
    "                raise Exception(\"Invalid shape! Expected {0} or {1} columns, got {2}\".format(self.nbins,self.n_good_bins, hd.shape[1]))\n",
    "\n",
    "        # remove bad bins\n",
    "        if not is_cleaned and self.remove_identical_bins:\n",
    "            hd = self.remove_bad_bins(hd)\n",
    "\n",
    "        # normalize each row\n",
    "        if self.normalize:\n",
    "            norms = np.sum(hd, axis=1)\n",
    "            tile = np.tile(norms, self.n_good_bins).reshape(self.n_good_bins, -1).T\n",
    "            hd = np.divide(hd, tile, out=np.zeros_like(hd), where=tile!=0)\n",
    "\n",
    "        if len(init_shape) == 1:\n",
    "            hd = hd.reshape(hd.size,)\n",
    "        return hd\n",
    "\n",
    "    def fit_transform(self, hd):\n",
    "        self.fit(hd)\n",
    "        return self.transform(hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQMPCA(object):\n",
    "    \"\"\"Class to perform PCA specifically on HistCollection objects\"\"\"\n",
    "\n",
    "    def __init__(self, use_standard_scaler=False, norm_cut=norm_cut, sse_ncomps=None):\n",
    "        \"\"\"Initialize the DQMPCA\n",
    "\n",
    "        -use_standard_scalar determines whether to use standard scaling\n",
    "          (zero mean, unit stdev) before feeding into a PCA. This helps\n",
    "          for some histograms, but hurts for others\n",
    "        \"\"\"\n",
    "        if use_standard_scaler:\n",
    "            self.pca = Pipeline(\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"pca\", PCA())\n",
    "                )\n",
    "        else:\n",
    "            self.pca = PCA()\n",
    "\n",
    "        self.use_standard_scaler = use_standard_scaler\n",
    "        self.norm_cut = norm_cut\n",
    "        self.sse_ncomps = sse_ncomps\n",
    "\n",
    "        self.__is_fit = False\n",
    "\n",
    "    @property\n",
    "    def sse_ncomps(self):\n",
    "        return self.__sse_ncomps\n",
    "\n",
    "    @sse_ncomps.setter\n",
    "    def sse_ncomps(self, sse):\n",
    "        if sse is not None and not isinstance(sse, tuple) and not isinstance(sse, list):\n",
    "            raise Exception(\"illigal sse_ncomps value. Should be None or a list/tuple of ints\")\n",
    "        self.__sse_ncomps = sse\n",
    "\n",
    "    def _check_fit(self):\n",
    "        if not self.__is_fit:\n",
    "            raise Exception(\"Must fit the DQMPCA before calling transform\")\n",
    "\n",
    "    def fit(self, hdata):\n",
    "        if isinstance(hdata, HistCollection):\n",
    "            self._hist_cleaner = hdata.hist_cleaner\n",
    "            cleaned = hdata.hdata\n",
    "            norms = hdata.norms\n",
    "            \n",
    "        else:\n",
    "            self._hist_cleaner = HistCleaner()\n",
    "            self._hist_cleaner.fit(hdata)\n",
    "            cleaned = self._hist_cleaner.transform(hdata)\n",
    "            norms = np.sum(cleaned, axis=1)\n",
    "\n",
    "        cleaned = cleaned[norms>self.norm_cut, :]\n",
    "        self.pca.fit(cleaned)        \n",
    "        self.__is_fit = True\n",
    "\n",
    "        if self.sse_ncomps is not None:\n",
    "            self.sse_cuts = {}\n",
    "            for ncomp in self.sse_ncomps:\n",
    "                self.sse_cuts[ncomp] = []\n",
    "                sses = self.sse(cleaned, ncomp)\n",
    "                for pct in np.arange(1,101):\n",
    "                    self.sse_cuts[ncomp].append(np.percentile(sses, pct))\n",
    "    \n",
    "    def transform(self, hdata):\n",
    "        \"\"\"Transform a set of histograms with the trained PCA\"\"\"\n",
    "        self._check_fit()\n",
    "        if isinstance(hdata, HistCollection):\n",
    "            cleaned = hdata.hdata\n",
    "        else:\n",
    "            cleaned = self._hist_cleaner.transform(hdata)        \n",
    "        return self.pca.transform(cleaned)\n",
    "        \n",
    "    def inverse_transform(self, xf, n_components=3, restore_bad_bins=False):\n",
    "        self._check_fit()\n",
    "        xf = np.array(xf)\n",
    "        trunc = np.zeros((xf.shape[0], self._hist_cleaner.n_good_bins))\n",
    "        trunc[:,:n_components] = xf[:,:n_components]\n",
    "        ixf = self.pca.inverse_transform(trunc)\n",
    "        if not restore_bad_bins:\n",
    "            return ixf\n",
    "        else:\n",
    "            return self._hist_cleaner.restore_bad_bins(ixf)\n",
    "\n",
    "    def sse(self, hdata, n_components=3):\n",
    "        if isinstance(hdata, HistCollection):\n",
    "            cleaned = hdata.hdata\n",
    "        else:\n",
    "            cleaned = self._hist_cleaner.transform(hdata)        \n",
    "        xf = self.transform(cleaned)\n",
    "        ixf = self.inverse_transform(xf, n_components=n_components)\n",
    "        return np.sqrt(np.sum((ixf-cleaned)**2, axis=1))\n",
    "        \n",
    "    def score(self, hdata, n_components=3):\n",
    "        if not hasattr(self, \"sse_cuts\") or n_components not in self.sse_cuts:\n",
    "            raise Exception(\"must fit first with {0} in sse_ncomps\".format(n_components))\n",
    "        sse = self.sse(hdata, n_components)\n",
    "        return np.interp(sse, self.sse_cuts[n_components], np.arange(1,101))\n",
    "\n",
    "    @property\n",
    "    def explained_variance_ratio(self):\n",
    "        if self.use_standard_scaler:\n",
    "            return self.pca.named_steps[\"pca\"].explained_variance_ratio_\n",
    "        else:\n",
    "            return self.pca.explained_variance_ratio_\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        if self.use_standard_scaler:\n",
    "            return self.pca.named_steps[\"scaler\"].inverse_transform(self.pca.named_steps[\"pca\"].mean_)\n",
    "        else:\n",
    "            return self.pca.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot in plots:\n",
    "    runs = []\n",
    "    hists = []\n",
    "    dname = plot[0]\n",
    "    hname = plot[1]\n",
    "    \n",
    "    #For 2018\n",
    "    #fnames = []\n",
    "    #fnamesD =glob.glob(\"/eos/cms/store/group/comm_dqm/DQMGUI_data/Run2018/SingleMuon/*/DQM_V0001_R000*__SingleMuon__Run2018D-PromptReco-v2__DQMIO.root\")\n",
    "    #fnamesABC = glob.glob(\"/eos/cms/store/group/comm_dqm/DQMGUI_data/Run2018/SingleMuon/*/DQM_V0001_R000*__SingleMuon__Run2018*-17Sep2018-*__DQMIO.root\")\n",
    "    #fnames= fnamesD +fnamesABC\n",
    "    \n",
    "    #For 2017\n",
    "    #fnames = []\n",
    "    #fnames =glob.glob(\"/eos/cms/store/group/comm_dqm/DQMGUI_data/Run2017/SingleMuon/*/DQM_V0001_R000*__SingleMuon__Run2017*-17Nov2017-v1__DQMIO.root\")\n",
    "    \n",
    "    #For 2016\n",
    "    fnames = []\n",
    "    fnames = glob.glob(\"/eos/cms/store/group/comm_dqm/DQMGUI_data/Run2016/SingleMuon/*/DQM_V0001_R000*__SingleMuon__Run2016*-21Feb2020_UL2016_HIPM-v1__DQMIO.root\")\n",
    "    for fname in fnames:\n",
    "        run = int(fname.split(\"/\")[-1].split(\"__\")[0][-6:])\n",
    "        #Corrupted file\n",
    "        if run == 315267:\n",
    "            continue\n",
    "        f = uproot.open(fname)\n",
    "        #Fetch all the 1D histograms into a list\n",
    "        histograms =f[hpath.format(run)].allitems(filterclass=lambda cls: issubclass(cls, uproot_methods.classes.TH1.Methods))\n",
    "        for name, roothist in histograms:\n",
    "            name = name.decode(\"utf-8\")\n",
    "            name = name.replace(\";1\", \"\")\n",
    "            #Grab the 1D histogram we want\n",
    "            if (dname in name) and (hname in name): \n",
    "                h = roothist.numpy()\n",
    "                #Include only histograms that have enough events\n",
    "                if norm_cut is None or HistogramIntegral(h) >= norm_cut:\n",
    "                    print(True)\n",
    "                    if max_bins==None:\n",
    "                        nbins = len(h[0])\n",
    "                    else:\n",
    "                        nbins = min(len(h[0]), max_bins)\n",
    "                    hists.append(h[0])\n",
    "                    runs.append(run)\n",
    "    #Make rows even length if jagged\n",
    "    lens = [len(row) for row in hists]\n",
    "    maxlen = np.amax(lens)\n",
    "    if maxlen != np.amin(lens):\n",
    "        for i in range(len(hists)):\n",
    "            hists[i] = np.ndarray.tolist(hists[i])\n",
    "    hists = np.array(hists)\n",
    "    #Define extra infos such as run number, title, and luminosity\n",
    "    #To be updated: query lumi data from OMS\n",
    "    extra_info = {\"runs\":runs}\n",
    "    extra_info[\"title\"]  = title\n",
    "    if lumi_json is not None:\n",
    "        with open(lumi_json, 'rb') as fid:\n",
    "            ri = json.load(fid)\n",
    "        lumis = []\n",
    "        for run in runs:\n",
    "            if str(run) in ri:\n",
    "                A = ri[str(run)][\"Initial Lumi\"]\n",
    "                B = ri[str(run)][\"Ending Lumi\"]\n",
    "                if A<0.1 or B<0.1:\n",
    "                    lumis.append(0)\n",
    "                elif A==B:\n",
    "                    lumis.append(A)\n",
    "                else:\n",
    "                    lumis.append((A-B)/np.log(A/B))\n",
    "            else:\n",
    "                lumis.append(0)\n",
    "        extra_info[\"lumis\"] = np.array(lumis)\n",
    "    #Clean and Store Histogram Data Using Class Object\n",
    "    hc = HistCollection(hists, extra_info=extra_info)\n",
    "    #Fit Modified PCA with Queried Data and Save It\n",
    "    pca = DQMPCA(norm_cut=10000, sse_ncomps=(1,2,3))\n",
    "    pca.fit(hc)\n",
    "    pkl_dir = os.path.join(\"models\",str(year))\n",
    "    os.system(\"mkdir -p \"+pkl_dir)\n",
    "    pkl_filename = \"{0}_{1}.pkl\".format(dname, hname)\n",
    "    pkl_filename = os.path.join(pkl_dir,pkl_filename)\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        #Protocol can be set higher once the code is moved to Python 3\n",
    "        pickle.dump(pca, file,protocol = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
