{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import uproot\n",
    "import uproot_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01, num_iters=5000, init_weight_spread=1.0, batch_size=None):\n",
    "\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = 0.01\n",
    "        self.num_iters = num_iters\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.X = tf.placeholder(\"float\", [None, layer_sizes[0]])\n",
    "\n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "\n",
    "        self.initializeWeights(init_weight_spread)\n",
    "        self.encoder = self.getEncoder(self.X)\n",
    "        self.decoder = self.getDecoder(self.encoder)\n",
    "                \n",
    "    def initializeWeights(self, weight_spread):\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            a = 4.0*np.sqrt(6.0/(self.layer_sizes[i] + self.layer_sizes[i-1]))\n",
    "            self.weights[\"encoder_\"+str(i)] = tf.Variable(tf.random_uniform([self.layer_sizes[i], self.layer_sizes[i+1]], -a, a))\n",
    "            a = 4.0*np.sqrt(6.0/(self.layer_sizes[-i-1] + self.layer_sizes[-i-2]))\n",
    "            self.weights[\"decoder_\"+str(i)] = tf.Variable(tf.random_uniform([self.layer_sizes[-i-1], self.layer_sizes[-i-2]], -a, a))\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            self.biases[\"encoder_\"+str(i)] = tf.Variable(tf.zeros([self.layer_sizes[i+1]]))\n",
    "            self.biases[\"decoder_\"+str(i)] = tf.Variable(tf.zeros([self.layer_sizes[-i-2]]))\n",
    "\n",
    "    def getEncoder(self, x):\n",
    "        ls = [x]\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            ls.append(tf.nn.sigmoid(tf.add(tf.matmul(ls[i], self.weights['encoder_'+str(i)]),\n",
    "                                           self.biases['encoder_'+str(i)])))\n",
    "        return ls[-1]\n",
    "        \n",
    "    def getDecoder(self, x):\n",
    "        ls = [x]\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            ls.append(tf.nn.sigmoid(tf.add(tf.matmul(ls[i], self.weights['decoder_'+str(i)]),\n",
    "                                           self.biases['decoder_'+str(i)])))\n",
    "        return ls[-1]\n",
    "\n",
    "    def train(self, sess, X, batch_size, output_every=None):\n",
    "        self.y_pred = self.decoder\n",
    "        self.y_true = self.X\n",
    "        self.loss = tf.reduce_mean(tf.pow(self.y_true - self.y_pred, 2))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(1, self.num_iters+1):\n",
    "            if self.batch_size is None:\n",
    "                batch_X = X\n",
    "            else:\n",
    "                batch_X = X[np.random.choice(np.arange(X.shape[0]), batch_size, replace=False), :]\n",
    "\n",
    "            _, l = sess.run([self.optimizer, self.loss], feed_dict={self.X:batch_X})\n",
    "\n",
    "            if output_every is not None and (i==1 or i%output_every==0):\n",
    "                print('Step {0}: Batch loss: {1}'.format(i, l))\n",
    "\n",
    "        return l\n",
    "\n",
    "    def run(self, sess, X, justEncoder=False):\n",
    "        if justEncoder:\n",
    "            res = sess.run(self.encoder, feed_dict={self.X: X})\n",
    "        else:\n",
    "            res = sess.run(self.decoder, feed_dict={self.X: X})\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Query and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HistogramIntegral returns the total number of events\n",
    "def HistogramIntegral(hist):\n",
    "    return sum(hist[0][i] for i in range(len(hist[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "hists = []\n",
    "year = 2018\n",
    "#norm_cut = None\n",
    "norm_cut = 10000\n",
    "max_bins = None\n",
    "title = None\n",
    "lumi_json = None\n",
    "plot = \"Segments/hSTimeCombined\"\n",
    "dname = plot.split('/')[0]\n",
    "hname = plot.split('/')[1]\n",
    "hpath = \"DQMData/Run {}/CSC/Run summary/CSCOfflineMonitor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if year ==2018:\n",
    "        fnamesD =glob.glob(\"/eos/cms/store/group/comm_dqm/DQMGUI_data/Run2018/SingleMuon/*/DQM_V0001_R000*__SingleMuon__Run2018D-PromptReco-v2__DQMIO.root\")\n",
    "        fnamesABC = glob.glob(\"/eos/cms/store/group/comm_dqm/DQMGUI_data/Run2018/SingleMuon/*/DQM_V0001_R000*__SingleMuon__Run2018*-17Sep2018-*__DQMIO.root\")\n",
    "        fnames= fnamesD +fnamesABC\n",
    "        \n",
    "if year ==2017:\n",
    "    fnames = []\n",
    "    fnames =glob.glob(\"/eos/cms/store/group/comm_dqm/DQMGUI_data/Run2017/SingleMuon/*/DQM_V0001_R000*__SingleMuon__Run2017*-17Nov2017-v1__DQMIO.root\")\n",
    "    \n",
    "if year ==2016:\n",
    "    fnames = glob.glob(\"/eos/cms/store/group/comm_dqm/DQMGUI_data/Run2016/SingleMuon/*/DQM_V0001_R000*__SingleMuon__Run2016*-21Feb2020_UL2016_HIPM-v1__DQMIO.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in fnames:\n",
    "    run = int(fname.split(\"/\")[-1].split(\"__\")[0][-6:])\n",
    "    #Corrupted file\n",
    "    if run == 315267:\n",
    "        continue\n",
    "    f = uproot.open(fname)\n",
    "    #Fetch all the 1D histograms into a list\n",
    "    histograms =f[hpath.format(run)].allitems(filterclass=lambda cls: issubclass(cls, uproot_methods.classes.TH1.Methods))\n",
    "    for name, roothist in histograms:\n",
    "        name = name.decode(\"utf-8\")\n",
    "        name = name.replace(\";1\", \"\")\n",
    "        #Grab the 1D histogram we want\n",
    "        if plot == name: \n",
    "            h = roothist.numpy()\n",
    "            #Include only histograms that have enough events\n",
    "            if norm_cut is None or HistogramIntegral(h) >= norm_cut:\n",
    "                if max_bins==None:\n",
    "                    nbins = len(h[0])\n",
    "                else:\n",
    "                    nbins = min(len(h[0]), max_bins)\n",
    "                hists.append(h[0])\n",
    "                runs.append(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
